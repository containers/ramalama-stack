name: Test LLS Integration

on:
  workflow_dispatch:
    inputs:
      inference_model:
        description: Model to download and inference via RamaLama
        required: false
        default: llama3.2:3b
  schedule:
    - cron: '0 11 * * *' # Runs at 11AM UTC every morning

env:
  LC_ALL: en_US.UTF-8

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  test-lls-integration:
    name: test-lls-integration
    runs-on: ubuntu-latest
    env:
      INFERENCE_MODEL: ${{ inputs.inference_model || 'llama3.2:3b' }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@f4a75cfd619ee5ce8d5b864b0d183aff3c69b55a # v2.13.1
        with:
          egress-policy: audit

      - name: Checkout containers/ramalama-stack
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          # https://github.com/actions/checkout/issues/249
          fetch-depth: 0

      - name: Install uv
        uses: astral-sh/setup-uv@557e51de59eb14aaaba2ed9621916900a91d50c6 # v6.6.1
        with:
          python-version: "3.12"

      - name: Set Up Environment and Install Dependencies
        run: |
          uv venv

          # install packaged version of ramalama-stack
          uv pip install ramalama-stack

          # update llama-stack version to main branch
          uv pip install git+https://github.com/meta-llama/llama-stack.git@main

          # temporary hack for file writing that should be done by the pip setup script
          # https://github.com/containers/ramalama-stack/issues/53
          mkdir -p ~/.llama/distributions/ramalama/
          cp -r $GITHUB_WORKSPACE/src/ramalama_stack/providers.d/ ~/.llama/
          cp $GITHUB_WORKSPACE/src/ramalama_stack/ramalama-run.yaml ~/.llama/distributions/ramalama/ramalama-run.yaml

      - name: Run 'test-build.sh'
        run: $GITHUB_WORKSPACE/tests/test-build.sh

      - name: Cache Ramalama store
        id: ramalama-store-cache
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809 # v4.2.4
        with:
          path: ~/.local/share/ramalama
          key: ramalama-store-${{ env.INFERENCE_MODEL }}

      - name: Download model to serve with Ramalama
        if: ${{ steps.ramalama-store-cache.outputs.cache-hit != 'true' }}
        run: uv run ramalama pull ${{ env.INFERENCE_MODEL }}

      - name: Run 'test-external-providers.sh'
        run: $GITHUB_WORKSPACE/tests/test-external-providers.sh

      - name: Upload logs
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: always()
        with:
          name: logs-test-lls-integration
          retention-days: 5
          path: |
            **/*.log
